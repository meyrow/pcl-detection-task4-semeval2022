{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final_predict_roberta_5.ipynb","provenance":[{"file_id":"1c975G_MbObmbAifhVb9_X57bQ3QdtM_c","timestamp":1643622033027},{"file_id":"1lSqMUCl9vPS5isYG57fvyztwTe7BYUI_","timestamp":1643227908444}],"collapsed_sections":[],"authorship_tag":"ABX9TyMCmTaR60JKJpoUtKDoIaj8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NaaRqsWEeGS","executionInfo":{"status":"ok","timestamp":1643640877632,"user_tz":-120,"elapsed":26876,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}},"outputId":"fad0060a-d92d-48d3-f66f-824130356b59"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"UQ2qvkIoEB3U","executionInfo":{"status":"ok","timestamp":1643640879451,"user_tz":-120,"elapsed":1840,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}}},"outputs":[],"source":["!cp gdrive/MyDrive/bertModel/bertTest.pkl ."]},{"cell_type":"code","source":["import pandas as pd\n","dataExam = pd.read_pickle(\"bertTest.pkl\")\n","dataExam.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"0zHjVRK1ReIf","executionInfo":{"status":"ok","timestamp":1643640879454,"user_tz":-120,"elapsed":29,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}},"outputId":"72ba4967-e2c0-496a-e8bd-75ae66f365d6"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-18b80f7c-38d1-49e3-b213-49fbd827985a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>par_id</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>t_0</td>\n","      <td>in the meantime conservatives are working to w...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>t_1</td>\n","      <td>in most poor households with no education chil...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>t_2</td>\n","      <td>the real question is not whether immigration i...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>t_3</td>\n","      <td>in total the country s immigrant population ha...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>t_4</td>\n","      <td>members of the church which is part of ken cop...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>t_5</td>\n","      <td>to ensure that priority agriculture programme ...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>t_6</td>\n","      <td>the deportees stepped off their flight from el...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>t_7</td>\n","      <td>pims staffer who raped disabled girl at icu wa...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>t_9</td>\n","      <td>i conclude yes the general feeling generated i...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>t_10</td>\n","      <td>after enduring discrimination in the allmale e...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18b80f7c-38d1-49e3-b213-49fbd827985a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-18b80f7c-38d1-49e3-b213-49fbd827985a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-18b80f7c-38d1-49e3-b213-49fbd827985a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["  par_id                                           sentence\n","0    t_0  in the meantime conservatives are working to w...\n","1    t_1  in most poor households with no education chil...\n","2    t_2  the real question is not whether immigration i...\n","3    t_3  in total the country s immigrant population ha...\n","4    t_4  members of the church which is part of ken cop...\n","5    t_5  to ensure that priority agriculture programme ...\n","6    t_6  the deportees stepped off their flight from el...\n","7    t_7  pims staffer who raped disabled girl at icu wa...\n","8    t_9  i conclude yes the general feeling generated i...\n","9   t_10  after enduring discrimination in the allmale e..."]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["dataExam.sentence.values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2yJJIrCSl9h","executionInfo":{"status":"ok","timestamp":1643640879457,"user_tz":-120,"elapsed":25,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}},"outputId":"cb9e1119-a24a-4b11-c22e-d0b7296a8c40"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['in the meantime conservatives are working to weaken clinton and drive down her numbers in early voting states where she is increasingly vulnerable they are in effect doing sanders s dirty work for him while he avoids scrutiny ',\n","       'in most poor households with no education children are a matter of routine the house maid industry is a sorry tale of dysfunctional families the norm of these families is to make their women and children work while the men do not work are on drugs and either just abuse their wives or produce more children ',\n","       'the real question is not whether immigration is good for the country or bad for the country the real question is the intent of the immigrant it is a subtle point and one that is very easy to miss ',\n","       ...,\n","       'of europe and they re still going to deal with the eu it s not the end of the world folks then again given how the liberal media is obsessed about race we should nt be shocked that they re smearing the entire leave camp as a bunch of xenophobic troglodytes',\n","       'this moral battle informed the recent defections those who belong to that pdp mode of thought could find no permanent comfort in walking the path of progressive reform and progress all the things we have inaugurated such as schoolfeeding programs for poor pupils social security for poor families affordable housing programs greater access to credit for small businesses and greater access to education and health care these things the defectors coul',\n","       'd not well abide they detested president buhari s treasury single account tsa innovation because it barred them from misdirecting funds into a maze of unaudited accounts from which they could siphon as they pleased buhari cut off their clandestine illicit spigot '],\n","      dtype=object)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["model_dir = F\"/content/gdrive/MyDrive/roberta_5_preprocessing_imbalance\""],"metadata":{"id":"P_jPSwnwFHsl","executionInfo":{"status":"ok","timestamp":1643640879458,"user_tz":-120,"elapsed":17,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZztJf_TGLP7","executionInfo":{"status":"ok","timestamp":1643640884118,"user_tz":-120,"elapsed":4677,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}},"outputId":"19778dfa-e2e0-4ceb-cca9-1de4809740cb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"code","source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tx6v5pFmGMBi","executionInfo":{"status":"ok","timestamp":1643640897805,"user_tz":-120,"elapsed":13704,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}},"outputId":"63ae54b3-dd8f-40e2-e814-ed38bc49440c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla P100-PCIE-16GB\n"]}]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"42BRRrdwF0xN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643640905493,"user_tz":-120,"elapsed":7716,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}},"outputId":"041b38a5-13ff-464c-df10-1d6ebafd1413"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\n","\u001b[K     |████████████████████████████████| 3.5 MB 4.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 63.5 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,>=0.10.1\n","  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n","\u001b[K     |████████████████████████████████| 6.8 MB 42.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 72.3 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 7.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.1\n"]}]},{"cell_type":"code","source":["from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer\n","MODEL_CLASSES = {\n","    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n","}\t\n","\n","# Config class and load a trained model and vocabulary \n","config_class, model_class, tokenizer_class = MODEL_CLASSES['roberta']\n","model = model_class.from_pretrained(model_dir)\n","tokenizer = tokenizer_class.from_pretrained(model_dir)\n","\n","# Copy the model to the GPU.\n","model.to(device)"],"metadata":{"id":"0S3Qr3ChFZDy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643640914135,"user_tz":-120,"elapsed":8673,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}},"outputId":"f364926b-d4e0-4ddb-ed5f-21c4c0bd8c68"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RobertaForSequenceClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","      (position_embeddings): Embedding(514, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (classifier): RobertaClassificationHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, SequentialSampler, DataLoader\n","# Report the number of sentences.\n","print('Number of test sentences: {:,}\\n'.format(dataExam.shape[0]))\n","\n","# Create sentence and label lists\n","sentences = dataExam.sentence.values\n","\n","#labels = df.label.values\n","\n","# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sent in sentences:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence.\n","    #   (2) Prepend the `[CLS]` token to the start.\n","    #   (3) Append the `[SEP]` token to the end.\n","    #   (4) Map tokens to their IDs.\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens.\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,                      # Sentence to encode.\n","                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                        max_length = 512,           # Pad & truncate all sentences.\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks.\n","                        return_tensors = 'pt',     # Return pytorch tensors.\n","                   )\n","    \n","    # Add the encoded sentence to the list.    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    # And its attention mask (simply differentiates padding from non-padding).\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors.\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","batch_size = 16  \n","\n","# Create the DataLoader.\n","prediction_data = TensorDataset(input_ids, attention_masks)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-EgcNifOcsk","executionInfo":{"status":"ok","timestamp":1643640918336,"user_tz":-120,"elapsed":4227,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}},"outputId":"d650b4e2-4b80-4d27-de6e-5a665dc0cf19"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Number of test sentences: 4,075\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2257: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions = []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions.\n","      result = model(b_input_ids, \n","                     token_type_ids=None, \n","                     attention_mask=b_input_mask,\n","                     return_dict=True)\n","\n","  logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","\n","print('    DONE.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Om1NH81xHbCD","executionInfo":{"status":"ok","timestamp":1643640992917,"user_tz":-120,"elapsed":74588,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}},"outputId":"3e39081e-38cb-42d5-b256-1fe0c058fe2d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicting labels for 4,075 test sentences...\n","    DONE.\n"]}]},{"cell_type":"code","source":["import math\n","intLoop = int(math.ceil(len(sentences)/batch_size))"],"metadata":{"id":"mOkhe_a5ZPZT","executionInfo":{"status":"ok","timestamp":1643640992918,"user_tz":-120,"elapsed":38,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","pred_labels_list = []\n","\n","# For each input batch...\n","for i in range(intLoop):\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  #print(f'i: {i}')\n","  #print(f'Prediction: {pred_labels_i}')\n","  pred_labels_list.extend(pred_labels_i)"],"metadata":{"id":"JtJcy1TTVlRF","executionInfo":{"status":"ok","timestamp":1643640992919,"user_tz":-120,"elapsed":35,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# helper function to save predictions to an output file\n","def labels2file(p, outf_path):\n","\twith open(outf_path,'w') as outf:\n","\t\tfor pi in p:\n","\t\t\toutf.write(str(pi)+'\\n')"],"metadata":{"id":"N42Dn14QcEbs","executionInfo":{"status":"ok","timestamp":1643640992919,"user_tz":-120,"elapsed":35,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import os\n","# first, we need to create the res/ and ref/ folders, which the evaluator expects\n","!mkdir res"],"metadata":{"id":"tDuBmGJOdwLB","executionInfo":{"status":"ok","timestamp":1643640992920,"user_tz":-120,"elapsed":34,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["labels2file(pred_labels_list, os.path.join('res/', 'roberta_task1.txt'))"],"metadata":{"id":"Gs2sxC-ad0f6","executionInfo":{"status":"ok","timestamp":1643640992921,"user_tz":-120,"elapsed":34,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["dataExam['pedict_label'] = pred_labels_list"],"metadata":{"id":"MCZ-g2RbhqzR","executionInfo":{"status":"ok","timestamp":1643640992922,"user_tz":-120,"elapsed":33,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["dataExam.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lUjDnHx600p0","executionInfo":{"status":"ok","timestamp":1643640992923,"user_tz":-120,"elapsed":33,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}},"outputId":"1346567d-0c6c-4c50-91fc-6e9698aab628"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4075, 3)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["!cp gdrive/MyDrive/classicmodels/allTest.pkl ."],"metadata":{"id":"J5XfRO8l8pjn","executionInfo":{"status":"ok","timestamp":1643640993980,"user_tz":-120,"elapsed":1084,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["allExam = pd.read_pickle(\"allTest.pkl\")\n","allExam.head(10)"],"metadata":{"id":"bHj-dPN6892y","colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"status":"ok","timestamp":1643640993982,"user_tz":-120,"elapsed":24,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}},"outputId":"ee9d01d0-78c7-42f3-aad5-f8ad57c1b7f4"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-ddb1fe26-5408-4fbb-a458-8dba76b7f2f5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>par_id</th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>t_0</td>\n","      <td>in the meantime conservatives are working to w...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>t_1</td>\n","      <td>in most poor households with no education chil...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>t_2</td>\n","      <td>the real question is not whether immigration i...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>t_3</td>\n","      <td>in total the country s immigrant population ha...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>t_4</td>\n","      <td>members of the church which is part of ken cop...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>t_5</td>\n","      <td>to ensure that priority agriculture programme ...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>t_6</td>\n","      <td>the deportees stepped off their flight from el...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>t_7</td>\n","      <td>pims staffer who raped disabled girl at icu wa...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>t_9</td>\n","      <td>i conclude yes the general feeling generated i...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>t_10</td>\n","      <td>after enduring discrimination in the allmale e...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ddb1fe26-5408-4fbb-a458-8dba76b7f2f5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ddb1fe26-5408-4fbb-a458-8dba76b7f2f5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ddb1fe26-5408-4fbb-a458-8dba76b7f2f5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["  par_id                                           sentence\n","0    t_0  in the meantime conservatives are working to w...\n","1    t_1  in most poor households with no education chil...\n","2    t_2  the real question is not whether immigration i...\n","3    t_3  in total the country s immigrant population ha...\n","4    t_4  members of the church which is part of ken cop...\n","5    t_5  to ensure that priority agriculture programme ...\n","6    t_6  the deportees stepped off their flight from el...\n","7    t_7  pims staffer who raped disabled girl at icu wa...\n","8    t_9  i conclude yes the general feeling generated i...\n","9   t_10  after enduring discrimination in the allmale e..."]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["allPar = allExam.par_id.values\n","labelAll = []\n","for parAll in allPar:\n","  mask = (dataExam['par_id'] == parAll)\n","  df_i = dataExam.loc[mask]\n","  labelAll.extend(str(max(df_i.pedict_label.values)))\n"],"metadata":{"id":"_e7IaW7O9Jcp","executionInfo":{"status":"ok","timestamp":1643640996686,"user_tz":-120,"elapsed":2720,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["len(labelAll)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sCRGpCdy-bbX","executionInfo":{"status":"ok","timestamp":1643640996687,"user_tz":-120,"elapsed":16,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}},"outputId":"489ef4da-ad60-41c9-a9be-18a36d7983f8"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3832"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["labels2file(labelAll, os.path.join('res/', 'roberta_5_task1.txt'))"],"metadata":{"id":"Fv2Sx1ZG_SPF","executionInfo":{"status":"ok","timestamp":1643640996689,"user_tz":-120,"elapsed":11,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["!cp /content/res/*.txt -r gdrive/MyDrive/bertModel/final/resBert/"],"metadata":{"id":"vibVHYuj_1H4","executionInfo":{"status":"ok","timestamp":1643641074089,"user_tz":-120,"elapsed":1206,"user":{"displayName":"ilan python","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLCRuHvuejnKEDiOM6ItLqiAqk-jFtZIMJJhjK=s64","userId":"01881011149054702533"}}},"execution_count":26,"outputs":[]}]}